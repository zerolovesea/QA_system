import streamlit as st
from langchain.chat_models import AzureChatOpenAI
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain_core.documents import Document
import os
from dotenv import load_dotenv

st.set_page_config(page_title="åŸºäºçŸ¥è¯†åº“å’Œå¤§æ¨¡å‹çš„é—®ç­”ç³»ç»Ÿ", page_icon="ğŸ“ˆ")
st.markdown("# åŸºäºçŸ¥è¯†åº“å’Œå¤§æ¨¡å‹çš„é—®ç­”ç³»ç»Ÿ")
st.sidebar.markdown('''
                    # é¢„è®­ç»ƒæ¨¡å‹

å‰é¢æåˆ°åŸºäºç»Ÿè®¡çš„æ£€ç´¢æ–¹æ³•å¹¶ä¸èƒ½ä»¥ç¬¦åˆè¯­ä¹‰çš„æ–¹å¼è¾“å‡ºå›ç­”ï¼Œè€Œæ˜¯è°ƒç”¨åŸæ–‡ã€‚é‚£ä¹ˆç°åœ¨æˆ‘ä»¬æœ‰äº†ä¼šè¯´äººè¯çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½ä¸èƒ½å°†å®ƒä»¬ç»“åˆåœ¨ä¸€èµ·å‘¢ï¼Ÿ

åŸºäºè¿™ä¸ªæ€æƒ³ï¼Œå‡ºç°äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ã€‚é€šè¿‡ç»“åˆè¾“å…¥é—®é¢˜å’Œæ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼Œå¤§è¯­è¨€æ¨¡å‹å°†å¾—åˆ°çš„å†…å®¹è¿›è¡ŒåŒ…è£…ï¼Œä½¿å¾—å…¶èƒ½å¤Ÿè¾“å‡ºåˆç†çš„ï¼Œç¬¦åˆè¯­ä¹‰çš„å›ç­”ã€‚

é€šè¿‡Langchainå’ŒChromaDBè¿›è¡Œäº†ç®€å•çš„å®ç°ã€‚            
                    ''')


load_dotenv()


llm = AzureChatOpenAI(
                    openai_api_base=os.getenv("OPENAI_API_BASE"),
                    openai_api_version=os.getenv("OPENAI_API_VERSION"),
                    deployment_name=os.getenv("DEPLOYMENT_NAME"),
                    temperature=os.getenv("TEMPERATURE"),
                    openai_api_key=os.getenv("OPENAI_API_KEY"),
                    openai_api_type=os.getenv("OPENAI_API_TYPE"),
                    streaming=os.getenv("STREAMING")
)
# å…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
question_file = st.file_uploader("è¯·é€‰æ‹©è¯­æ–™", type=["txt"])

if question_file is not None:
    try:
        question_data = question_file.getvalue().decode("utf-8")
    except UnicodeDecodeError:
        try:
            question_data = question_file.getvalue().decode("gbk")
        except:
            st.error("æ— æ³•è§£ç æ–‡ä»¶å†…å®¹ã€‚è¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç æˆ–ä½¿ç”¨å…¶ä»–ç¼–ç å°è¯•ã€‚")
    
    print(question_data)
    sentences = question_data.split('ã€‚')
    sentences = [s.strip() for s in sentences if s.strip()]

    st.markdown("å°†ä¼šå¯¹è¯­æ–™è¿›è¡Œåˆ‡å—ï¼Œå¹¶å‘é‡åŒ–ã€‚è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")
    documents = [Document(page_content=s, metadata={'source': 'wiki'}) for s in sentences]

    print(documents)
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)

    embeddings = SentenceTransformerEmbeddings()
    db = Chroma.from_documents(docs, embeddings)
    st.markdown("è¯­æ–™åŠ è½½å®Œæˆã€‚")
    st.markdown("---")
    vectordb = Chroma(persist_directory='db', embedding_function=embeddings)
    retriever = db.as_retriever(search_type="mmr")

    user_question = st.text_input("è¯·è¾“å…¥é—®é¢˜ï¼š")
    docs = db.similarity_search(user_question)

    if st.button("æ‰§è¡Œå‘é‡æŸ¥è¯¢"):
        st.markdown('**æ£€ç´¢åˆ°çš„æœ€ç›¸ä¼¼çš„å†…å®¹ï¼š**')
        st.markdown(docs[0].page_content)


    if st.button("å‘æ¨¡å‹è¿›è¡Œæé—®"):
        from langchain.prompts import ChatPromptTemplate

        template = """ä½ æ˜¯ä¸€ä¸ªç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚
        ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
        å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œåªéœ€è¯´ä½ ä¸çŸ¥é“ã€‚
        æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ï¼Œå¹¶ä¿æŒç­”æ¡ˆç®€æ´ã€‚
        é—®é¢˜ï¼š{question}
        ä¸Šä¸‹æ–‡ï¼š{context}
        ç­”æ¡ˆï¼š
        """
        prompt = ChatPromptTemplate.from_template(template)
        from langchain.schema.runnable import RunnablePassthrough
        from langchain.schema.output_parser import StrOutputParser

        rag_chain = (
            {"context": retriever,  "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        query = user_question
        st.markdown('**æ¨¡å‹çš„å›ç­”ï¼š**')        
        st.markdown(rag_chain.invoke(query))


st.markdown('''   
            ---    
            ## ç‰¹ç‚¹
                        
            å¤§è¯­è¨€æ¨¡å‹ç»è¿‡äº†ä¸Šäº¿é—®ç­”è¯­æ–™çš„å­¦ä¹ ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å­¦åˆ°è¿‡çš„å†…å®¹ï¼Œæ‰¾åˆ°æœ€ç¬¦åˆäººç±»é€»è¾‘çš„ä¸‹ä¸€ä¸ªè¾“å‡ºã€‚ç”±äºé—®é¢˜æ˜¯å¯¹Pythonçš„è¯¢é—®ï¼Œå®ƒä»ä¸Šæ–‡ä¸­æ‰¾åˆ°äº†æœ€æœ‰å¯èƒ½å‡ºç°çš„å›ç­”ï¼Œå¹¶ä»¥äººç±»èƒ½å¤Ÿç†è§£çš„æ–¹å¼è¿›è¡Œè¾“å‡ºã€‚

            ç„¶è€Œï¼Œç”±äºç”Ÿæˆè¯­è¨€æ¨¡å‹ä»¥ç”Ÿæˆç¬¦åˆè¯­ä¹‰çš„å¥å­ä¸ºç›®çš„ï¼Œå› æ­¤å®ƒæ— æ³•åˆ¤æ–­è¾“å‡ºçš„å†…å®¹æ˜¯å¦å‡†ç¡®ï¼Œè¿™ä¹Ÿæ˜¯å®ƒç›®å‰å¤‡å—è¯Ÿç—…çš„ç¼ºé™·ä¹‹ä¸€ã€‚
            ''')


st.markdown('''
            ---           
            æˆ‘ä»¬é™„ä¸Šæºç å¦‚ä¸‹ï¼š
            ```python
            from langchain.embeddings import SentenceTransformerEmbeddings
            from langchain.text_splitter import CharacterTextSplitter
            from langchain.vectorstores import Chroma
            from langchain.document_loaders import TextLoader

            # ä»æœ¬åœ°å¯¼å…¥è¯­æ–™
            loader = TextLoader('state_of_the_union.txt')
            documents = loader.load()

            # å°†æ–‡æœ¬è¿›è¡Œåˆ‡å—
            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
            docs = text_splitter.split_documents(documents)

            # å°†è¯­æ–™åµŒå…¥åå­˜å…¥å‘é‡æ•°æ®åº“
            embeddings = SentenceTransformerEmbeddings()
            db = Chroma.from_documents(docs, embeddings)
            
                        
            query = "What did the president say about Ketanji Brown Jackson"
            
            docs = db.similarity_search_with_score(query)
            docs[0]
                        
            from langchain.prompts import ChatPromptTemplate

            template = """You are an assistant for question-answering tasks.
            Use the following pieces of retrieved context to answer the question.
            If you don't know the answer, just say that you don't know.
            Use three sentences maximum and keep the answer concise.
            Question: {question}
            Context: {context}
            Answer:
            """
            prompt = ChatPromptTemplate.from_template(template)
                        
            from langchain.schema.runnable import RunnablePassthrough
            from langchain.schema.output_parser import StrOutputParser

            rag_chain = (
                {"context": retriever,  "question": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
            )

            query = "What did the president say about Justice Breyer"
            rag_chain.invoke(query)

            vectordb = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory='db')
            vectordb.persist()
            vectordb = None
                        
            vectordb = Chroma(persist_directory='db', embedding_function=embeddings)
            retriever = db.as_retriever(search_type="mmr")
            retriever.get_relevant_documents(query)[0]

                        ''')