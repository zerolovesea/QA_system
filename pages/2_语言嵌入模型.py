import streamlit as st
from text2vec import SentenceModel, cos_sim, semantic_search


st.set_page_config(page_title="æ–‡æœ¬åµŒå…¥æ¨¡å‹", page_icon="ğŸ“ˆ")
st.markdown("# æ–‡æœ¬åµŒå…¥æ¨¡å‹")
st.sidebar.markdown('''
                    # æ–‡æœ¬åµŒå…¥æ¨¡å‹
å‰é¢æåˆ°çš„TF-IDFä½¿ç”¨äº†åŸºäºç»Ÿè®¡çš„æ–‡æœ¬åµŒå…¥æ–¹å¼ï¼Œåç»­éšç€NLPçš„å‘å±•ï¼Œåˆå‡ºç°äº†è¯­è¨€æ¨¡å‹ã€‚è¿™æ—¶å€™å·²ç»èƒ½é€šè¿‡è¯­è¨€æ¨¡å‹å°†æ–‡æœ¬åµŒå…¥ä¸ºæ›´é«˜ç»´çš„è¾“å…¥äº†ï¼Œä¸€å®šç¨‹åº¦ä¸Šä¹Ÿèƒ½å¤Ÿç†è§£è¯­ä¹‰ã€‚

æˆ‘ä»¬ä½¿ç”¨Text2Vecè¿›è¡Œäº†å®ç°ï¼Œå®ƒæ ¸å¿ƒä½¿ç”¨äº†transformersåº“çš„text2vec-base-chineseåµŒå…¥æ¨¡å‹ã€‚             
                    ''')

embedder = SentenceModel()

# å…è®¸ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
question_file = st.file_uploader("è¯·é€‰æ‹©é—®é¢˜è¯­æ–™", type=["txt"])
if question_file is not None:
    question_data = question_file.getvalue().decode("utf-8")
    question_list = question_data.splitlines()
    
    st.text("è½¬æ¢åçš„é—®é¢˜åˆ—è¡¨ï¼š")
    st.write(question_list)
    corpus_embeddings = embedder.encode(question_list)

answer_file = st.file_uploader("è¯·é€‰æ‹©ç­”æ¡ˆè¯­æ–™", type=["txt"])
if answer_file is not None:
    answer_data = answer_file.getvalue().decode("utf-8")
    answer_list = answer_data.splitlines()
    
    st.text("è½¬æ¢åçš„ç­”æ¡ˆåˆ—è¡¨ï¼š")
    st.write(answer_list)

top_k = st.number_input("é€‰æ‹© top_k çš„å€¼:", min_value=1, max_value=5, value=3, step=1)

if st.button("ä½¿ç”¨åµŒå…¥è¯­è¨€æ¨¡å‹å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œéå†"):
    progress_bar = st.progress(0)
    for index, query in enumerate(question_list):
        progress_bar.progress((index + 1) / len(question_list))

        query_embedding = embedder.encode(query)

        hits = semantic_search(query_embedding, corpus_embeddings, top_k=top_k )

        st.markdown("---")
        st.markdown(f"**Query:** {query}")
        st.markdown(f"\n\n**è¯­æ–™ä¸­æœ€ç›¸ä¼¼çš„{top_k}ä¸ªå›ç­”ï¼š**")
        hits = hits[0]  
        for hit in hits:
            st.markdown(f"- {question_list[hit['corpus_id']]}  (Score: **{hit['score']:.4f}**)")

st.markdown(''' 
---      
## ç‰¹ç‚¹
            
åœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œè¿™ç§è¯­è¨€æ¨¡å‹åªæ˜¯å¯¹ä¼ ç»ŸåŸºäºç»Ÿè®¡çš„åµŒå…¥æ–¹å¼è¿›è¡Œäº†æ”¹è‰¯ï¼Œæœ¬è´¨æ£€ç´¢ä¸Šå¹¶æ²¡æœ‰è„±ç¦»ç›¸ä¼¼åº¦çš„æ¡æ¢ã€‚å½“ç„¶ï¼Œåç»­Bertç­‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œæ–‡æœ¬ç»­å†™ç­‰ä»»åŠ¡ï¼Œä½†æ˜¯åœ¨é—®ç­”ä»»åŠ¡ä¸Šä»ç„¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆã€‚
''')

st.markdown('''
---           
æˆ‘ä»¬é™„ä¸Šæºç å¦‚ä¸‹ï¼š
```python
from text2vec import SentenceModel, cos_sim, semantic_search

# ä½¿ç”¨äº†transformersåº“çš„text2vec-base-chineseåµŒå…¥æ¨¡å‹
embedder = SentenceModel()

# è¯­æ–™æ ·æœ¬åº“
corpus = [
    'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
    'æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘—',
    'A man is eating food.',
    'A man is eating a piece of bread.',
    'The girl is carrying a baby.',
    'A man is riding a horse.',
    'A woman is playing violin.',
    'Two men pushed carts through the woods.',
    'A man is riding a white horse on an enclosed ground.',
    'A monkey is playing drums.',
    'A cheetah is running behind its prey.'
]

# å°†è¯­æ–™è¿›è¡ŒåµŒå…¥
corpus_embeddings = embedder.encode(corpus)
corpus_embeddings
            

queries = [
    'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
    'A man is eating pasta.',
    'Someone in a gorilla costume is playing a set of drums.',
    'A cheetah chases prey on across a field.']
            

for query in queries:
    query_embedding = embedder.encode(query)
    hits = semantic_search(query_embedding, corpus_embeddings, top_k=3)
    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nè¯­æ–™ä¸­æœ€ç›¸ä¼¼çš„ä¸‰ä¸ªå›ç­”ï¼š")
    hits = hits[0]  
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
            ''')